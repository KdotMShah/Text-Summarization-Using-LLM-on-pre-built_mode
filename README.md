# Text-Summarization-Using-LLM-on-pre-built_mode
The mT5 model was presented in mT5: A massively multilingual pre-trained text-to-text transformer by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.
Here, I have used the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset to host on Gradio and compute large texts to be sumamrized into one line. no further finetuning is performed on this model by me.
XL-SUM Dataset: https://huggingface.co/datasets/csebuetnlp/xlsum

CITATIONS:

"Large-Scale Multilingual Abstractive Summarization for 44 Languages"
  author = Hasan, Tahmid  and
      Bhattacharjee, Abhik  and
      Islam, Md. Saiful  and
      Mubasshir, Kazi  and
      Li, Yuan-Fang  and
      Kang, Yong-Bin  and
      Rahman, M. Sohel  and
      Shahriyar, Rifat
    
  booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"
      month = aug,
      year = "2021",
      address = "Online",
      publisher = "Association for Computational Linguistics",
      url = "https://aclanthology.org/2021.findings-acl.413",
      pages = "4693--4703"

OFFICIAL REPOSITORY OF THE PREBUILT MODEL:
https://github.com/csebuetnlp/xl-sum
